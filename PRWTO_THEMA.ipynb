{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the page has reached its end. It cannot scroll anymore\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 161>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapo katw article url \u001b[39m\u001b[38;5;124m'\u001b[39m , article_url)\n\u001b[0;32m    158\u001b[0m             tag_fun(article_url)\n\u001b[1;32m--> 161\u001b[0m \u001b[43mscroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscroll_pause_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(tag_list)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tag_list))\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mscroll\u001b[1;34m(driver, timeout)\u001b[0m\n\u001b[0;32m    155\u001b[0m     article_url \u001b[38;5;241m=\u001b[39m https \u001b[38;5;241m+\u001b[39m article_url\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapo katw article url \u001b[39m\u001b[38;5;124m'\u001b[39m , article_url)\n\u001b[1;32m--> 158\u001b[0m \u001b[43mtag_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mscroll.<locals>.tag_fun\u001b[1;34m(inside)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m    113\u001b[0m page \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(inside)\n\u001b[1;32m--> 114\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m tags \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m , class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagsCnt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m tags \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    397\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTMLParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\html\\parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\html\\parser.py:344\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:154\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m#print(\"START\", name)\u001b[39;00m\n\u001b[0;32m    153\u001b[0m sourceline, sourcepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetpos()\n\u001b[1;32m--> 154\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:714\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"Called by the tree builder when a new tag is encountered.\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m:param name: Name of the tag.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03mdon't call handle_endtag.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# print(\"Start tag %s: %s\" % (name, attrs))\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    718\u001b[0m          \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39msearch_tag(name, attrs))):\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:588\u001b[0m, in \u001b[0;36mBeautifulSoup.endData\u001b[1;34m(self, containerClass)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    584\u001b[0m        (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39msearch(current_data)):\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m containerClass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontainerClass\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m o \u001b[38;5;241m=\u001b[39m containerClass(current_data)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_was_parsed(o)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:499\u001b[0m, in \u001b[0;36mBeautifulSoup.string_container\u001b[1;34m(self, base_class)\u001b[0m\n\u001b[0;32m    496\u001b[0m container \u001b[38;5;241m=\u001b[39m base_class \u001b[38;5;129;01mor\u001b[39;00m NavigableString\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# There may be a general override of NavigableString.\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m container \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\n\u001b[0;32m    500\u001b[0m     container, container\n\u001b[0;32m    501\u001b[0m )\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# On top of that, we may be inside a tag that needs a special\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# container class.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_container_stack \u001b[38;5;129;01mand\u001b[39;00m container \u001b[38;5;129;01mis\u001b[39;00m NavigableString:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "url = input('Give me eplease the URL')\n",
    "tname = input('print the tag name')\n",
    "a = 'Prwto_Thema'\n",
    "b = '_Wdate'\n",
    "csv = '.csv'\n",
    "simple = a+tname+csv\n",
    "wd = a+tname+b+csv\n",
    "\n",
    "driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))\n",
    "date_list = []\n",
    "tag_list = []\n",
    "\n",
    "stop_date = 20230731\n",
    "scroll_pause_time = 2.6\n",
    "\n",
    "#       Poll\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)\n",
    "disagree = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div/div/div[2]/div/button[2]\"))).click()\n",
    "\n",
    "flag = True\n",
    "inside_flag = True\n",
    "def scroll(driver, timeout):\n",
    "    global flag\n",
    "    global stop_date\n",
    "    global inside_flag\n",
    "    global tag_list\n",
    "    global date_list\n",
    "    scroll_pause_time = timeout\n",
    "    # its the nr of scrollings that after that i check the date.\n",
    "    nr_of_scrollings = 2\n",
    "    changable_nr_of_scrollings = nr_of_scrollings\n",
    "    #inside_flag = True\n",
    "\n",
    "    def date_check_fun():\n",
    "        global inside_flag\n",
    "\n",
    "        # In time, when the function gets inside the main function the coment section here(2 lines)\n",
    "        # will need to replace the 2 bellow them.\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'lxml')\n",
    "        articles_place = soup.find('div' , class_ = \"listLoop\")\n",
    "        dates = articles_place.find_all('div' , class_ = \"wrp\")\n",
    "        for i in dates:\n",
    "            date = i.find('time')\n",
    "            date = date.get_text()\n",
    "            date = date.split(\",\")\n",
    "            date = date[0].split(\".\")\n",
    "            date.reverse()\n",
    "            date = \"\".join(date)\n",
    "            date = int(date)\n",
    "            #print(date)\n",
    "\n",
    "        if date < stop_date :\n",
    "                print('the date is smaller. Programm Must STOP')\n",
    "                inside_flag = False\n",
    "                return inside_flag\n",
    "    \n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while inside_flag:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # If heights are the same it will exit the function\n",
    "            #flag=False\n",
    "            print('the page reaches the end for the first time. It cannot scroll anymore')\n",
    "            break\n",
    "        last_height = new_height\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # i want every some scroll downs to check the date\n",
    "        changable_nr_of_scrollings = changable_nr_of_scrollings - 1\n",
    "        #nr_of_scrollings = nr_of_scrollings - 1\n",
    "        if changable_nr_of_scrollings == 0:\n",
    "            date_check_fun()\n",
    "            #print('on the outer/mother function the inside_flag is  ' , inside_flag)\n",
    "            #nr_of_scrollings = 2 \n",
    "            changable_nr_of_scrollings = nr_of_scrollings\n",
    "\n",
    "    #   THIS PART OF CODE I WORK now. The apo epanw functions properly.\n",
    "    \n",
    "    def tag_fun(inside):\n",
    "        global tag_list\n",
    "        inside_tag_list = []\n",
    "\n",
    "        from bs4 import BeautifulSoup\n",
    "        import requests\n",
    "\n",
    "        page = requests.get(inside)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        tags = soup.find('div' , class_ = \"tagsCnt\")\n",
    "        tags = tags.find_all('a')\n",
    "        for a in tags:\n",
    "            tag = a.get_text()\n",
    "            tag = tag.replace('\\r' , \"\")\n",
    "            tag = tag.replace('\\n' ,  \"\")\n",
    "            tag = tag.strip()\n",
    "            inside_tag_list.append(tag)   \n",
    "        tag_list.append(inside_tag_list)\n",
    "    \n",
    "\n",
    "    #inside_flag = True\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source,'lxml')\n",
    "\n",
    "    articles_place = soup.find('div' , class_ = \"listLoop\")\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    for i in articles:\n",
    "        date = i.find('div' , class_ = \"wrp\")\n",
    "        date = date.find('time')\n",
    "        date = date.get_text()\n",
    "        date = date.split(\",\")\n",
    "        date = date[0].split(\".\")\n",
    "        date.reverse()\n",
    "        date = \"\".join(date)\n",
    "        date = int(date)\n",
    "        #print(date)\n",
    "\n",
    "        if date < stop_date :\n",
    "            print('the date is smaller. I need to terminate the whole programm')\n",
    "            break\n",
    "        else:\n",
    "            article_url = i.find('div' , class_ = 'heading')\n",
    "            article_url = article_url.find('a')\n",
    "            article_url = article_url.get('href')\n",
    "            date_list.append(date)\n",
    "            #print(article_url)\n",
    "            https = \"https://www.protothema.gr\"\n",
    "            if article_url.find(https)<0:\n",
    "                article_url = https + article_url\n",
    "                print('apo katw article url ' , article_url)\n",
    "            \n",
    "            tag_fun(article_url)\n",
    "        \n",
    "\n",
    "scroll(driver , scroll_pause_time)\n",
    "\n",
    "print(tag_list)\n",
    "print(len(tag_list))\n",
    "print(len(date_list))\n",
    "print('the last date is ' , date_list[-1])\n",
    "\n",
    "import pandas as pd\n",
    "# df_costliness = pd.DataFrame(tag_list,date_list)\n",
    "# df_costliness.to_csv('costliness.csv')\n",
    "\n",
    "#print(tag_list)\n",
    "#print(date_list)\n",
    "\n",
    "new_tag_list = []\n",
    "for i in range(len(tag_list)):\n",
    "    for j in range(len(tag_list[i])):\n",
    "        #if tag_list[i][j] not in new_tag_list:\n",
    "       new_tag_list.append(tag_list[i][j])\n",
    "#print(new_tag_list)\n",
    "\n",
    "res = {}\n",
    "for i in new_tag_list:\n",
    "    res[i]=new_tag_list.count(i)\n",
    "print(res)\n",
    "\n",
    "resultList = new_list = list(map(list, res.items()))\n",
    "# printing the resultant list of a dictionary key-values\n",
    "print(resultList)\n",
    "\n",
    "df_costliness = pd.DataFrame(resultList)\n",
    "df_costliness.to_csv(simple, header=False)\n",
    "\n",
    "\n",
    "# constructs the date, ready for tableau, with the form day, month, year with dots. Saves it in the var 'ndlist'\n",
    "ddate_list=date_list\n",
    "ttag_list = tag_list\n",
    "\n",
    "ndlist = []\n",
    "for i in ddate_list:\n",
    "    i=str(i)\n",
    "    y=i[:4]\n",
    "    m=i[4:6]\n",
    "    d=i[6:]\n",
    "    # here, i am gona crate the date with dots and the system: day,month, year:\n",
    "    ddate = d+'.'+m+'.'+y\n",
    "    ndlist.append(ddate)\n",
    "print(ddate_list)\n",
    "print(ndlist)\n",
    "\n",
    "n=0\n",
    "date_con_list = []\n",
    "for i in ttag_list:\n",
    "    n=n+1\n",
    "    for tag in i:\n",
    "        inside=[]\n",
    "        inside.append(tag)\n",
    "        inside.append(ddate_list[n-1])\n",
    "        date_con_list.append(inside)\n",
    "print(date_con_list)\n",
    "\n",
    "df_connected = pd.DataFrame(date_con_list)\n",
    "df_connected.to_csv(wd, header=False)\n",
    "\n",
    "\n",
    "print(len(tag_list))\n",
    "print(len(date_list))\n",
    "print(tag_list)\n",
    "print('the last date collected is ' , date_list[-1])\n",
    "print('the nr of the tags mentioned is ' , len(resultList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the date is smaller. Programm Must STOP\n",
      "apo katw article url  https://www.protothema.grhttps://www.ygeiamou.gr/%ce%b5%ce%b9%ce%b4%ce%ae%cf%83%ce%b5%ce%b9%cf%82/%cf%80%ce%bf%ce%bb%ce%b9%cf%84%ce%b9%ce%ba%ce%ae-%cf%85%ce%b3%ce%b5%ce%af%ce%b1%cf%82/393226/techniti-noimosini-o-protagonistis-tou-5ou-sinedriou-ygeiamou/\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.protothema.grhttps', port=443): Max retries exceeded with url: //www.ygeiamou.gr/%CE%B5%CE%B9%CE%B4%CE%AE%CF%83%CE%B5%CE%B9%CF%82/%CF%80%CE%BF%CE%BB%CE%B9%CF%84%CE%B9%CE%BA%CE%AE-%CF%85%CE%B3%CE%B5%CE%AF%CE%B1%CF%82/393226/techniti-noimosini-o-protagonistis-tou-5ou-sinedriou-ygeiamou/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E7790E2F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001E7790E2F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:785\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    783\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 785\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.protothema.grhttps', port=443): Max retries exceeded with url: //www.ygeiamou.gr/%CE%B5%CE%B9%CE%B4%CE%AE%CF%83%CE%B5%CE%B9%CF%82/%CF%80%CE%BF%CE%BB%CE%B9%CF%84%CE%B9%CE%BA%CE%AE-%CF%85%CE%B3%CE%B5%CE%AF%CE%B1%CF%82/393226/techniti-noimosini-o-protagonistis-tou-5ou-sinedriou-ygeiamou/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E7790E2F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 175>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapo katw article url \u001b[39m\u001b[38;5;124m'\u001b[39m , article_url)\n\u001b[0;32m    172\u001b[0m             tag_fun(article_url)\n\u001b[1;32m--> 175\u001b[0m \u001b[43mscroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscroll_pause_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(tag_list)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tag_list))\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mscroll\u001b[1;34m(driver, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m     article_url \u001b[38;5;241m=\u001b[39m https \u001b[38;5;241m+\u001b[39m article_url\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapo katw article url \u001b[39m\u001b[38;5;124m'\u001b[39m , article_url)\n\u001b[1;32m--> 172\u001b[0m \u001b[43mtag_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mscroll.<locals>.tag_fun\u001b[1;34m(inside)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\requests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m'\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    648\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.protothema.grhttps', port=443): Max retries exceeded with url: //www.ygeiamou.gr/%CE%B5%CE%B9%CE%B4%CE%AE%CF%83%CE%B5%CE%B9%CF%82/%CF%80%CE%BF%CE%BB%CE%B9%CF%84%CE%B9%CE%BA%CE%AE-%CF%85%CE%B3%CE%B5%CE%AF%CE%B1%CF%82/393226/techniti-noimosini-o-protagonistis-tou-5ou-sinedriou-ygeiamou/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E7790E2F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "url = input('Give me eplease the URL')\n",
    "tname = input('print the tag name')\n",
    "a = 'Prwto_Thema'\n",
    "b = '_Wdate'\n",
    "csv = '.csv'\n",
    "simple = a+tname+csv\n",
    "wd = a+tname+b+csv\n",
    "\n",
    "driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))\n",
    "date_list = []\n",
    "tag_list = []\n",
    "\n",
    "stop_date = 20230731\n",
    "scroll_pause_time = 1.8\n",
    "\n",
    "#       Poll\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)\n",
    "disagree = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div/div/div[2]/div/button[2]\"))).click()\n",
    "\n",
    "flag = True\n",
    "inside_flag = True\n",
    "def scroll(driver, timeout):\n",
    "    global flag\n",
    "    global stop_date\n",
    "    global inside_flag\n",
    "    global tag_list\n",
    "    global date_list\n",
    "    scroll_pause_time = timeout\n",
    "    # its the nr of scrollings that after that i check the date.\n",
    "    nr_of_scrollings = 2\n",
    "    changable_nr_of_scrollings = nr_of_scrollings\n",
    "    #inside_flag = True\n",
    "\n",
    "    def date_check_fun():\n",
    "        global inside_flag\n",
    "\n",
    "        # In time, when the function gets inside the main function the coment section here(2 lines)\n",
    "        # will need to replace the 2 bellow them.\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'lxml')\n",
    "        articles_place = soup.find('div' , class_ = \"listLoop\")\n",
    "        dates = articles_place.find_all('div' , class_ = \"wrp\")\n",
    "        for i in dates:\n",
    "            date = i.find('time')\n",
    "            date = date.get_text()\n",
    "            date = date.split(\",\")\n",
    "            date = date[0].split(\".\")\n",
    "            date.reverse()\n",
    "            date = \"\".join(date)\n",
    "            date = int(date)\n",
    "            #print(date)\n",
    "\n",
    "        if date < stop_date :\n",
    "                print('the date is smaller. Programm Must STOP')\n",
    "                inside_flag = False\n",
    "                return inside_flag\n",
    "    \n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while inside_flag:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # If heights are the same it will exit the function\n",
    "            #flag=False\n",
    "            time.sleep(scroll_pause_time)\n",
    "# second try\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height==last_height:\n",
    "                print('the page reaches the end for the first time. It cannot scroll anymore')\n",
    "                break\n",
    "            last_height=new_height\n",
    "# second try\n",
    "        last_height = new_height\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # i want every some scroll downs to check the date\n",
    "        changable_nr_of_scrollings = changable_nr_of_scrollings - 1\n",
    "        #nr_of_scrollings = nr_of_scrollings - 1\n",
    "        if changable_nr_of_scrollings == 0:\n",
    "            date_check_fun()\n",
    "            #print('on the outer/mother function the inside_flag is  ' , inside_flag)\n",
    "            #nr_of_scrollings = 2 \n",
    "            changable_nr_of_scrollings = nr_of_scrollings\n",
    "\n",
    "    #   THIS PART OF CODE I WORK now. The apo epanw functions properly.\n",
    "    \n",
    "    def tag_fun(inside):\n",
    "        global tag_list\n",
    "        inside_tag_list = []\n",
    "\n",
    "        from bs4 import BeautifulSoup\n",
    "        import requests\n",
    "\n",
    "        page = requests.get(inside)\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            tags = soup.find('div' , class_ = \"tagsCnt\")\n",
    "            tags = tags.find_all('a')\n",
    "            for a in tags:\n",
    "                tag = a.get_text()\n",
    "                tag = tag.replace('\\r' , \"\")\n",
    "                tag = tag.replace('\\n' ,  \"\")\n",
    "                tag = tag.strip()\n",
    "                inside_tag_list.append(tag)   \n",
    "            tag_list.append(inside_tag_list)\n",
    "        except:\n",
    "            print('Propably  have found a brokenarticle. Its url is ' , inside)\n",
    "\n",
    "    #inside_flag = True\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source,'lxml')\n",
    "\n",
    "    articles_place = soup.find('div' , class_ = \"listLoop\")\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    for i in articles:\n",
    "        date = i.find('div' , class_ = \"wrp\")\n",
    "        date = date.find('time')\n",
    "        date = date.get_text()\n",
    "        date = date.split(\",\")\n",
    "        date = date[0].split(\".\")\n",
    "        date.reverse()\n",
    "        date = \"\".join(date)\n",
    "        date = int(date)\n",
    "        #print(date)\n",
    "\n",
    "        if date < stop_date :\n",
    "            print('the date is smaller. I need to terminate the whole programm')\n",
    "            break\n",
    "        else:\n",
    "            article_url = i.find('div' , class_ = 'heading')\n",
    "            article_url = article_url.find('a')\n",
    "            article_url = article_url.get('href')\n",
    "            date_list.append(date)\n",
    "            #print(article_url)\n",
    "            https = \"https://www.protothema.gr\"\n",
    "            if article_url.find(https)<0:\n",
    "                print('xlasmeno article url ' , article_url)\n",
    "            \n",
    "            tag_fun(article_url)\n",
    "        \n",
    "\n",
    "scroll(driver , scroll_pause_time)\n",
    "\n",
    "print(tag_list)\n",
    "print(len(tag_list))\n",
    "print(len(date_list))\n",
    "print('the last date is ' , date_list[-1])\n",
    "\n",
    "import pandas as pd\n",
    "# df_costliness = pd.DataFrame(tag_list,date_list)\n",
    "# df_costliness.to_csv('costliness.csv')\n",
    "\n",
    "#print(tag_list)\n",
    "#print(date_list)\n",
    "\n",
    "new_tag_list = []\n",
    "for i in range(len(tag_list)):\n",
    "    for j in range(len(tag_list[i])):\n",
    "        #if tag_list[i][j] not in new_tag_list:\n",
    "       new_tag_list.append(tag_list[i][j])\n",
    "#print(new_tag_list)\n",
    "\n",
    "res = {}\n",
    "for i in new_tag_list:\n",
    "    res[i]=new_tag_list.count(i)\n",
    "print(res)\n",
    "\n",
    "resultList = new_list = list(map(list, res.items()))\n",
    "# printing the resultant list of a dictionary key-values\n",
    "print(resultList)\n",
    "\n",
    "df_costliness = pd.DataFrame(resultList)\n",
    "df_costliness.to_csv(simple, header=False)\n",
    "\n",
    "\n",
    "# constructs the date, ready for tableau, with the form day, month, year with dots. Saves it in the var 'ndlist'\n",
    "ddate_list=date_list\n",
    "ttag_list = tag_list\n",
    "\n",
    "ndlist = []\n",
    "for i in ddate_list:\n",
    "    i=str(i)\n",
    "    y=i[:4]\n",
    "    m=i[4:6]\n",
    "    d=i[6:]\n",
    "    # here, i am gona crate the date with dots and the system: day,month, year:\n",
    "    ddate = d+'.'+m+'.'+y\n",
    "    ndlist.append(ddate)\n",
    "print(ddate_list)\n",
    "print(ndlist)\n",
    "\n",
    "n=0\n",
    "date_con_list = []\n",
    "for i in ttag_list:\n",
    "    n=n+1\n",
    "    for tag in i:\n",
    "        inside=[]\n",
    "        inside.append(tag)\n",
    "        inside.append(ddate_list[n-1])\n",
    "        date_con_list.append(inside)\n",
    "print(date_con_list)\n",
    "\n",
    "df_connected = pd.DataFrame(date_con_list)\n",
    "df_connected.to_csv(wd, header=False)\n",
    "\n",
    "\n",
    "print(len(tag_list))\n",
    "print(len(date_list))\n",
    "print(tag_list)\n",
    "print('the last date collected is ' , date_list[-1])\n",
    "print('the nr of the tags mentioned is ' , len(resultList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
